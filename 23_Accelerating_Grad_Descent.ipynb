{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Momentum\n",
    "\n",
    "* Gradient Descent doesnt takes into account of previous gradients, prob with that is:\n",
    "    * if it had hit plateau it could get stuck\n",
    "    * Derivatives are noisy -> Fluctuations -> Takes longer time to converge \n",
    "\n",
    "* Momentum overcomes this disadvantage by denoising the gradients.\n",
    "\n",
    "* The idea is to denoise derivative using exponential weighting average (smoothen derivatives) -> give more weightage to recent updates compared to the previous update.\n",
    "\n",
    "* This accelerates convergence bcos it shows which direction to go and also makes it come out of plateaus. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Equations\n",
    "\n",
    "* Usual Gradient Descent:\n",
    "\n",
    "    1. $ w\\  += \\eta * dx $\n",
    "\n",
    "* Momentum:\n",
    "    1. $ v = \\alpha * v - \\eta * dx $\n",
    "\n",
    "    2. $ w \\ += v $\n",
    "\n",
    "    * We are adding momentum to the weight update step\n",
    "\n",
    "    * The momentum at each step is computed using its previous updates\n",
    "\n",
    "    * $\\alpha$ is % of gradient maintained, extra hyperparameter (~0.9) : importance of how much to consider previous time step"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Equations\n",
    "\n",
    "* Usual Gradient Descent:\n",
    "\n",
    "    1. $ w_{k+1} = w_k - \\eta * \\nabla L(w_{old}) $\n",
    "\n",
    "* Momentum:\n",
    "    1. $ v_{k+1} = \\alpha * v_k - \\eta * \\nabla L(w_{old}) $\n",
    "\n",
    "    2. $ w_{k+1} = w_k + v_{k+1} $\n",
    "\n",
    "    * We are adding momentum to the weight update step\n",
    "\n",
    "    * The momentum at each step is computed using its previous updates\n",
    "\n",
    "    * $\\alpha$ is % of gradient maintained, extra hyperparameter (~0.9) : importance of how much to consider previous time step"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## How it speeds convergence\n",
    "\n",
    "* Consider two scenarios:\n",
    "\n",
    "1. Weight update with gradients only :\n",
    "    *  $ w_{k+1} = w_k - \\eta * \\nabla L(w_{old}) $\n",
    "\n",
    "    *  It will be oriented in a direction\n",
    "\n",
    "2. Weight update with momentum only :\n",
    "    *  $ w_{k+1} = w_k - \\alpha * v_{k} $\n",
    "\n",
    "    *  It will be oriented in another direction\n",
    "\n",
    "* If we combine both, we get :\n",
    "    *  $ w_{k+1} = w_k - (\\alpha * v_{k} + \\eta * \\nabla L(w_{old})) $\n",
    "\n",
    "    * This will move in direction which denoises the update\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}