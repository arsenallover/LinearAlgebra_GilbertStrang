{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## SVD Recap\n",
    "\n",
    "* Any rectangular matrix A can be factorized and represented as r rank 1 features\n",
    "    * $ A = u \\sum v^T = \\sigma_1\\  u_1\\  v_1^T + \\sigma_2\\  u_2 \\ v_2^T + .. + \\sigma_r\\ u_r\\ v_r^T $\n",
    "    \n",
    "    * u can restrict to K features to avoid learning all information in data -> overfitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Vector Norm:\n",
    "\n",
    "* L2 : $||v||_2 = \\sqrt {|v_1|^2 + ... + |v_n|^2} $\n",
    "\n",
    "* L1 : $||v||_1 = |v_1| + ... + |v_n| $\n",
    "\n",
    "* $L_{\\inf} : max |v_i| $\n",
    "\n",
    "* Properties of Norm :\n",
    "    * ||cv|| = |c|*||v||\n",
    "\n",
    "    * ||v + w|| =< ||v|| + ||w|| Triangle Inequality (where hyp is always less than the sum of other two)\n",
    "\n",
    "    * $||Qv||_2^2 = (Qv)^T(Qv) = (Q^TQ)||v||_2^2 = ||v||_2^2 $\n",
    "    This says when you multiply by orthogonal matrix with any vector and take its norm, it doesnt change. The hypotenuse changes direction through Q but nt the length\n",
    "\n",
    "## Matrix Norm:\n",
    "\n",
    "* $||A||_2 = \\sigma_1 $\n",
    "\n",
    "* Frobenius Norm: Using all single elements in matrix\n",
    "\n",
    "$||A||_F = \\sqrt {|a_{11}|^2 + |a_{12}|^2... + |a_{mn}|^2} $\n",
    "\n",
    "* $||A||_{nuclear} = \\sigma_1 + \\sigma_2 + ... + \\sigma_r $\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Eckart-Young Theorem\n",
    "\n",
    "* If B has rank k then, $ || A - B || >= || A - A_K ||$\n",
    "\n",
    "* $A_K$ is reduced martix of A with rank k (If diag it has only K diag non-zero elements )\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PCA is not least squares\n",
    "\n",
    "* Error : \n",
    "    * In least squares you find the error from data to the line (perpendicular to x axis)\n",
    "\n",
    "    * In PCA you find the error from data to the line (perpendicular to line itself)\n",
    "\n",
    "* Equation:\n",
    "    * In LS, we use normal equation : $ A^T A x = A^T b $\n",
    "    * In PCA, we use SVD and $\\sigma$'s"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PCA\n",
    "\n",
    "* Center Data\n",
    "\n",
    "* Covariance Matrix : Tells varaince not only within variables but also between variables\n",
    "\n",
    "    * Sample cov matrix : $ \\frac{A A^T}{N-1} $\n",
    "\n",
    "* In PCA, we are to find the best line such that $ x_2 = c*x_1$. It turns out the value c is $\\sigma_1$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}